{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecf6862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp311-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in ./old/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./old/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, networkx, filelock, torch\n",
      "Successfully installed filelock-3.12.2 mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.0.1 typing-extensions-4.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.2-cp311-cp311-macosx_11_0_arm64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp311-cp311-macosx_11_0_arm64.whl (229 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.40.0-cp311-cp311-macosx_10_9_universal2.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp311-cp311-macosx_11_0_arm64.whl (63 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.25.0-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./old/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-10.0.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing<3.1,>=2.3.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./old/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./old/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.40.0 kiwisolver-1.4.4 matplotlib-3.7.2 numpy-1.25.0 pillow-10.0.0 pyparsing-3.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "c8043312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3ce67c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = open('names.txt','r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "0053f330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "print(names_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8554a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "char_to_int['.'] = 0\n",
    "int_to_char[0] = '.'\n",
    "\n",
    "for char_num in range(ord('a'),ord('z')+1):\n",
    "    integer_representation = char_num-ord('a')+1\n",
    "    char_to_int[chr(char_num)] = integer_representation\n",
    "    int_to_char[integer_representation] = chr(char_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "0853e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "embed_size = 6\n",
    "\n",
    "state_size = 20\n",
    "\n",
    "pre_hidden_size = 200\n",
    "\n",
    "hidden_size = 100\n",
    "\n",
    "vocab_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "84d731ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "char_vector_size = 2\n",
    "input_contexts , input_labels = [[] for _ in range(5)], [[] for _ in range(5)]\n",
    "\n",
    "for name in names_list:\n",
    "    log_padded_length = math.ceil(math.log(len(name))/math.log(2))\n",
    "    padded_length = int(2**log_padded_length)\n",
    "    context = [char_to_int[char] for char in name]  + [0] + [vocab_size-1]*(padded_length-len(name)) \n",
    "    input_contexts[log_padded_length].append(context)\n",
    "#   instead of using labels, we can compare the result to the character vectors to find a likely match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "3a836871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1404, 0.1485, 0.0116, 0.0155, 0.0241, 0.0895, 0.0040, 0.0084, 0.0334,\n",
       "        0.0776, 0.0127, 0.0221, 0.0612, 0.0291, 0.0803, 0.0348, 0.0045, 0.0012,\n",
       "        0.0557, 0.0355, 0.0244, 0.0137, 0.0113, 0.0041, 0.0031, 0.0428, 0.0105,\n",
       "        0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = [0 for i in range(vocab_size-1)]\n",
    "\n",
    "for log_len in range(1,5):\n",
    "    contexts = input_contexts[log_len]\n",
    "    for c in contexts:\n",
    "        for int_char in c:\n",
    "            if int_char<vocab_size-1:\n",
    "                freqs[int_char] += 1\n",
    "                \n",
    "freqs = torch.tensor(freqs+[0],dtype=torch.float64)\n",
    "freqs/=freqs.sum()\n",
    "\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "e63c25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = input_contexts[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "40819a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "all_char_embeds = torch.randn((vocab_size,embed_size))\n",
    "\n",
    "all_char_embeds[vocab_size-1] = 10**10\n",
    "\n",
    "w_ep = torch.randn((embed_size, pre_hidden_size)) * 0.5\n",
    "\n",
    "w_sp = torch.randn((state_size, pre_hidden_size)) * 0.5\n",
    "w_so = torch.randn((state_size, embed_size)) *0.5\n",
    "\n",
    "w_ph = torch.randn((pre_hidden_size, hidden_size)) * 0.5\n",
    "\n",
    "w_hs = torch.randn((hidden_size, state_size)) *0.5\n",
    "\n",
    "\n",
    "b_p = torch.zeros((pre_hidden_size,))\n",
    "b_h = torch.zeros((hidden_size,))\n",
    "b_s = torch.zeros((state_size,))\n",
    "b_o = torch.zeros((embed_size,))\n",
    "\n",
    "batch_norm_layer = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "init_state = torch.randn((state_size,))\n",
    "\n",
    "network_params = [init_state,w_ep,w_sp,w_ph,w_so,w_hs,b_p,b_h,b_s,b_o,all_char_embeds]\n",
    "\n",
    "network_params += [p for p in batch_norm_layer.parameters()]\n",
    "\n",
    "for p in network_params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "68feafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state_and_embed(states , embeds, hiddens, outputs, idx):\n",
    "    pre_hidden = 2*torch.sigmoid((states[idx] @ w_sp) + (embeds[idx] @ w_ep) + b_p) -1    \n",
    "    hiddens[idx] = batch_norm_layer(pre_hidden @ w_ph + b_h)\n",
    "    \n",
    "    states[idx+1] = 2*torch.sigmoid(hiddens[idx] @ w_hs + b_s) - 1\n",
    "    outputs[idx] = states[idx] @ w_so + b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "0ed9af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/ks60gt8d7bbbzmtsh2l4f8y00000gn/T/ipykernel_29664/955914535.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_contexts[i] = torch.tensor(input_contexts[i])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for i in range(len(input_contexts)):\n",
    "    input_contexts[i] = torch.tensor(input_contexts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "fef2cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 of 500000:  tensor(0.0151, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "1000 of 500000:  tensor(0.0152, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "1500 of 500000:  tensor(0.0152, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "2000 of 500000:  tensor(0.0151, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "2500 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "3000 of 500000:  tensor(0.0152, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "3500 of 500000:  tensor(0.0153, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "4000 of 500000:  tensor(0.0151, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "4500 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "5000 of 500000:  tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "5500 of 500000:  tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "6000 of 500000:  tensor(0.0152, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "6500 of 500000:  tensor(0.0153, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "7000 of 500000:  tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "7500 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "8000 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "8500 of 500000:  tensor(0.0148, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "9000 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "9500 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "10000 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "10500 of 500000:  tensor(0.0148, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "11000 of 500000:  tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "11500 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "12000 of 500000:  tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "12500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "13000 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "13500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "14000 of 500000:  tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "14500 of 500000:  tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "15000 of 500000:  tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "15500 of 500000:  tensor(0.0148, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "16000 of 500000:  tensor(0.0148, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "16500 of 500000:  tensor(0.0148, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "17000 of 500000:  tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "17500 of 500000:  tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "18000 of 500000:  tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "18500 of 500000:  tensor(0.0150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "19000 of 500000:  tensor(0.0148, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "19500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "20000 of 500000:  tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "20500 of 500000:  tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "21000 of 500000:  tensor(0.0145, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "21500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "22000 of 500000:  tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "22500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "23000 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "23500 of 500000:  tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "24000 of 500000:  tensor(0.0144, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "24500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "25000 of 500000:  tensor(0.0145, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "25500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "26000 of 500000:  tensor(0.0145, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "26500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "27000 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "27500 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "28000 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "28500 of 500000:  tensor(0.0141, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "29000 of 500000:  tensor(0.0144, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "29500 of 500000:  tensor(0.0144, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "30000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "30500 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "31000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "31500 of 500000:  tensor(0.0148, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "32000 of 500000:  tensor(0.0144, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "32500 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "33000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "33500 of 500000:  tensor(0.0144, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "34000 of 500000:  tensor(0.0144, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "34500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "35000 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "35500 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "36000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "36500 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "37000 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "37500 of 500000:  tensor(0.0144, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "38000 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "38500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "39000 of 500000:  tensor(0.0141, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "39500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "40000 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "40500 of 500000:  tensor(0.0140, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "41000 of 500000:  tensor(0.0140, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "41500 of 500000:  tensor(0.0140, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "42000 of 500000:  tensor(0.0140, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "42500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "43000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "43500 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "44000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "44500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "45000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "45500 of 500000:  tensor(0.0141, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "46000 of 500000:  tensor(0.0154, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "46500 of 500000:  tensor(0.0162, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "47000 of 500000:  tensor(0.0152, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "47500 of 500000:  tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "48000 of 500000:  tensor(0.0146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "48500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "49000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "49500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "50000 of 500000:  tensor(0.0141, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "50500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "51000 of 500000:  tensor(0.0141, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "51500 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "52000 of 500000:  tensor(0.0141, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "52500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "53000 of 500000:  tensor(0.0140, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53500 of 500000:  tensor(0.0140, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "54000 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "54500 of 500000:  tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "55000 of 500000:  tensor(0.0143, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "55500 of 500000:  tensor(0.0138, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "56000 of 500000:  tensor(0.0141, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "56500 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "57000 of 500000:  tensor(0.0136, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "57500 of 500000:  tensor(0.0138, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "58000 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "58500 of 500000:  tensor(0.0140, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "59000 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "59500 of 500000:  tensor(0.0137, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "60000 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "60500 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "61000 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "61500 of 500000:  tensor(0.0136, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "62000 of 500000:  tensor(0.0136, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "62500 of 500000:  tensor(0.0138, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "63000 of 500000:  tensor(0.0135, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "63500 of 500000:  tensor(0.0138, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "64000 of 500000:  tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[392], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m ones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(embeds \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1e10\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m (filtered\u001b[38;5;241m*\u001b[39madjustment_factors)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m network_params:\n\u001b[1;32m     51\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/PycharmProjects/old-rnn/old/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/old-rnn/old/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_tot = 0\n",
    "\n",
    "loops = 500000\n",
    "check_every = 500\n",
    "\n",
    "# training loop\n",
    "for i in range(loops):\n",
    "    log_idx = random.randint(1,4)\n",
    "#     log_idx = 4\n",
    "    batch_indices = torch.randint(0,input_contexts[log_idx].shape[0],(batch_size,))\n",
    "\n",
    "    batch = input_contexts[log_idx][batch_indices]\n",
    "\n",
    "    word_length = len(batch[0])\n",
    "\n",
    "    embeds = torch.transpose(all_char_embeds[batch],0,1)\n",
    "    \n",
    "    factors = torch.where(freqs > 0,1/(freqs+1e-18),0)\n",
    "    \n",
    "    factors[:-1] /= factors[:-1].mean()\n",
    "    \n",
    "    adjustment_factors = torch.transpose(factors[batch],0,1)\n",
    "    \n",
    "    adjustment_factors = adjustment_factors.view((word_length,batch_size,1))\n",
    "    \n",
    "    adjustment_factors[:-1] /= adjustment_factors[:-1].mean()\n",
    "#     print(adjustment_factors , embeds.shape)\n",
    "\n",
    "    states = [init_state] + [torch.zeros((batch_size,state_size,)) for _ in range(word_length-1)]\n",
    "\n",
    "    hiddens = [torch.zeros((batch_size,hidden_size,)) for _ in range(word_length-1)]\n",
    "\n",
    "    outputs = torch.zeros((word_length,batch_size,embed_size))\n",
    "\n",
    "    for j in range(word_length-1):\n",
    "        process_state_and_embed(states,embeds ,hiddens,outputs,j)\n",
    "\n",
    "    outputs[-1] = states[-1] @ w_so + b_o\n",
    "    \n",
    "    distances_sqd = (torch.cdist(outputs,all_char_embeds[:-1]) ** 2).sum(2,keepdim=True)\n",
    "        \n",
    "    filtered = torch.where(embeds != 1e10, ((outputs-embeds)**2 / distances_sqd), 0)\n",
    "    \n",
    "    ones = torch.where(embeds != 1e10, 1, 0)\n",
    "    \n",
    "    loss = (filtered*adjustment_factors).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    for p in network_params:\n",
    "        p.data -= 1e-3*p.grad\n",
    "        p.grad = None\n",
    "        \n",
    "    loss_tot+= loss/ adjustment_factors.sum()\n",
    "    if i % check_every==0 and i>0:\n",
    "        print(f\"{i} of {loops}: \" , loss_tot / (check_every))\n",
    "        loss_tot = 0\n",
    "#     print('o',outputs)\n",
    "#     print('e',embeds)\n",
    "#     print('wl',word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "17d8265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word):\n",
    "    word_length = len(word)\n",
    "    \n",
    "    embeds = all_char_embeds[word].view(word_length,1,embed_size)\n",
    "    \n",
    "    states = [init_state]+[torch.zeros((1,state_size,)) for _ in range(word_length-1)]\n",
    "\n",
    "    hiddens = [torch.zeros((1,hidden_size,)) for _ in range(word_length-1)]\n",
    "\n",
    "    outputs = torch.zeros((word_length,1,embed_size))\n",
    "\n",
    "    for i in range(word_length-1):\n",
    "        process_state_and_embed(states,embeds ,hiddens,outputs,i)\n",
    "\n",
    "    return states[-1] @ w_so + b_o\n",
    "\n",
    "\n",
    "def get_next(text):\n",
    "    word = [char_to_int[c] for c in text] +[0]\n",
    "\n",
    "    pred = predict(word)\n",
    "\n",
    "    pred = pred.view(1,embed_size)\n",
    "    dists = torch.cdist(pred,all_char_embeds[:-1])\n",
    "    dists = dists.view(-1)\n",
    "    dists = torch.exp(-10*dists)\n",
    "    dists/=dists.sum()\n",
    "    sampled = (torch.multinomial(dists,1)).item()\n",
    "    return int_to_char[sampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_st = ''\n",
    "\n",
    "batch_norm_layer.eval()\n",
    "\n",
    "for _ in range(100):\n",
    "    st = start_st\n",
    "    curr = 'a'\n",
    "    while curr!='.':\n",
    "        curr = get_next(st)\n",
    "        st+=curr\n",
    "    print(st)\n",
    "\n",
    "batch_norm_layer.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "088c8955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0844, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((all_char_embeds[char_to_int['k']]-all_char_embeds[char_to_int['c']])**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "16f2916a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.9781e+00, -2.0762e+00, -3.4334e+00,  2.4585e+00,  1.4191e+00,\n",
       "          6.3284e-01],\n",
       "        [ 2.8782e+00, -1.2092e+00, -1.2280e+00,  1.9302e+00,  1.4470e+00,\n",
       "          1.0097e+00],\n",
       "        [-1.4000e+00,  9.5057e-01,  5.8750e-01, -4.9056e-01, -4.7422e-01,\n",
       "          1.9161e-01],\n",
       "        [-1.2603e+00,  7.0542e-01,  7.4141e-01, -5.2271e-01, -7.6276e-01,\n",
       "          9.6872e-02],\n",
       "        [-1.3179e+00,  6.0693e-01,  4.3269e-01, -2.9754e-01, -5.7641e-01,\n",
       "          7.0558e-02],\n",
       "        [ 2.5471e+00, -8.5447e-01, -1.3418e+00,  1.7515e+00,  1.3425e+00,\n",
       "          9.6918e-01],\n",
       "        [-1.7320e+00,  9.7192e-01,  8.1608e-01, -9.3318e-01, -8.1058e-01,\n",
       "          7.7136e-02],\n",
       "        [-1.5648e+00,  7.5046e-01,  7.1344e-01, -5.1856e-01, -7.9058e-01,\n",
       "          3.2312e-01],\n",
       "        [ 1.2899e+00, -3.9766e-01, -3.1215e-01,  1.1563e+00,  5.1118e-01,\n",
       "          3.5854e-01],\n",
       "        [ 2.7570e+00, -2.2321e+00, -1.0240e+00,  1.7206e+00,  1.5739e+00,\n",
       "          7.8637e-01],\n",
       "        [-1.5944e+00,  7.5318e-01,  5.7706e-01, -4.8852e-01, -7.5572e-01,\n",
       "         -2.0941e-02],\n",
       "        [-2.1459e+00,  1.0428e+00,  8.0551e-01, -8.4968e-01, -7.8428e-01,\n",
       "         -1.7649e-01],\n",
       "        [-6.3829e-01,  4.8088e-01,  1.8654e-01,  3.9272e-01, -6.4293e-01,\n",
       "          6.8807e-02],\n",
       "        [-1.4496e+00,  9.4717e-01,  5.2809e-01, -5.4597e-01, -5.6420e-01,\n",
       "         -4.7823e-02],\n",
       "        [-1.7220e-01,  4.8551e-01,  4.1639e-02,  6.5642e-02, -2.5650e-01,\n",
       "         -1.4729e-01],\n",
       "        [ 2.6121e+00, -1.0316e+00, -9.2446e-01,  1.9536e+00,  6.8824e-01,\n",
       "          6.1715e-01],\n",
       "        [-1.4942e+00,  9.1826e-01,  6.1101e-01, -6.4903e-01, -6.9456e-01,\n",
       "          1.6227e-02],\n",
       "        [-2.1040e+00,  1.1020e+00,  9.6551e-01, -7.0075e-01, -9.4947e-01,\n",
       "         -3.8982e-02],\n",
       "        [-5.2410e-01,  1.4798e-01,  1.3853e-01,  1.9003e-01, -3.0851e-01,\n",
       "          2.7779e-01],\n",
       "        [-1.0758e+00,  4.3508e-01,  3.8606e-01, -3.2926e-01, -4.2928e-01,\n",
       "          9.8771e-02],\n",
       "        [-5.2323e-01,  1.9386e-01,  1.9637e-02,  1.0744e-01, -1.0446e-01,\n",
       "          3.8009e-01],\n",
       "        [ 2.3184e+00, -1.1620e+00, -1.5673e+00,  1.7262e+00,  8.9515e-01,\n",
       "          4.6350e-01],\n",
       "        [-1.7152e+00,  8.3900e-01,  9.0333e-01, -3.9708e-01, -9.1538e-01,\n",
       "         -9.5183e-02],\n",
       "        [-6.7973e-01,  4.1594e-01,  2.3355e-01,  5.7740e-02, -2.6333e-01,\n",
       "          9.2766e-02],\n",
       "        [-2.2641e+00,  1.2244e+00,  1.1284e+00, -1.2820e+00, -1.0800e+00,\n",
       "         -2.9311e-02],\n",
       "        [ 2.3685e+00, -2.3777e+00, -1.0749e+00,  1.9502e+00,  1.5166e+00,\n",
       "          4.3475e-01],\n",
       "        [-1.7775e+00,  7.8257e-01,  7.2931e-01, -7.2238e-01, -7.3543e-01,\n",
       "         -7.9175e-02],\n",
       "        [ 1.0000e+10,  1.0000e+10,  1.0000e+10,  1.0000e+10,  1.0000e+10,\n",
       "          1.0000e+10]], requires_grad=True)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_char_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea083d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
